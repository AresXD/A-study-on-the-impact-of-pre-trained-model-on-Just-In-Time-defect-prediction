test qt_10_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_10/qt/epoch_1_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [00:57<00:00, 11.22it/s]
Test data at Threshold 0.5 -- AUc: 0.63 Accuracy: 0.07, False Positives: 0.93, Precision: 0.07, Recall: 1.00
程序运行时间：82.57834 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_10/qt/epoch_2_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [00:57<00:00, 11.19it/s]
Test data at Threshold 0.5 -- AUc: 0.72 Accuracy: 0.07, False Positives: 0.93, Precision: 0.07, Recall: 1.00
程序运行时间：81.89143 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_10/qt/epoch_3_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [00:57<00:00, 11.18it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.07, False Positives: 0.93, Precision: 0.07, Recall: 1.00
程序运行时间：81.38295 s
test qt_100_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_100/qt/epoch_1_step_7.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.89it/s]
Test data at Threshold 0.5 -- AUc: 0.72 Accuracy: 0.07, False Positives: 0.93, Precision: 0.07, Recall: 1.00
程序运行时间：79.16284 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_100/qt/epoch_2_step_7.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.07, False Positives: 0.93, Precision: 0.07, Recall: 1.00
程序运行时间：80.96872 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_100/qt/epoch_3_step_7.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.87it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.59, False Positives: 0.87, Precision: 0.13, Recall: 0.82
程序运行时间：79.08696 s
test qt_500_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_500/qt/epoch_1_step_32.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.76, False Positives: 0.83, Precision: 0.17, Recall: 0.61
程序运行时间：78.93868 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_500/qt/epoch_2_step_32.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.89it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.74, False Positives: 0.83, Precision: 0.17, Recall: 0.65
程序运行时间：78.30428 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_500/qt/epoch_3_step_32.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.78, False Positives: 0.80, Precision: 0.20, Recall: 0.68
程序运行时间：78.28644 s
test qt_1000_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_1000/qt/epoch_1_step_63.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.89it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.75, False Positives: 0.82, Precision: 0.18, Recall: 0.66
程序运行时间：79.00618 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_1000/qt/epoch_2_step_63.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.72, False Positives: 0.84, Precision: 0.16, Recall: 0.68
程序运行时间：78.70315 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_1000/qt/epoch_3_step_63.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.89it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.47, False Positives: 0.90, Precision: 0.10, Recall: 0.83
程序运行时间：78.36762 s
test qt_2000_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_2000/qt/epoch_1_step_126.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.89it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.75, False Positives: 0.82, Precision: 0.18, Recall: 0.70
程序运行时间：80.12342 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_2000/qt/epoch_2_step_126.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.81 Accuracy: 0.55, False Positives: 0.88, Precision: 0.12, Recall: 0.87
程序运行时间：78.83164 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/roberta_2000/qt/epoch_3_step_126.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.61, False Positives: 0.86, Precision: 0.14, Recall: 0.82
程序运行时间：78.35517 s
test openstack_10_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_10/openstack/epoch_1_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:30<00:00, 10.88it/s]
Test data at Threshold 0.5 -- AUc: 0.47 Accuracy: 0.12, False Positives: 0.88, Precision: 0.12, Recall: 1.00
程序运行时间：39.91732 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_10/openstack/epoch_2_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:30<00:00, 10.87it/s]
Test data at Threshold 0.5 -- AUc: 0.61 Accuracy: 0.12, False Positives: 0.88, Precision: 0.12, Recall: 1.00
程序运行时间：40.13564 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_10/openstack/epoch_3_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:30<00:00, 10.99it/s]
Test data at Threshold 0.5 -- AUc: 0.68 Accuracy: 0.12, False Positives: 0.88, Precision: 0.12, Recall: 1.00
程序运行时间：39.22051 s
test openstack_100_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_100/openstack/epoch_1_step_7.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Traceback (most recent call last):
  File "RoBERTa.py", line 710, in <module>
    auc_score, A, E, P, R = evaluation_model(data=data, params=params)
  File "RoBERTa.py", line 646, in evaluation_model
    A, E, P, R=eval(all_label, all_predict, thresh=0.5)
  File "RoBERTa.py", line 483, in eval
    return (A, E, P, R)
UnboundLocalError: local variable 'A' referenced before assignment
Test data at Threshold 0.5 -- AUc: 100.0 Accuracy: 100.0, False Positives: 100.0, Precision: 100.0,Recall: 100.0
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_100/openstack/epoch_2_step_7.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.73it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.63, False Positives: 0.78, Precision: 0.22, Recall: 0.78
程序运行时间：38.31640 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_100/openstack/epoch_3_step_7.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.73it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.20, False Positives: 0.87, Precision: 0.13, Recall: 0.99
程序运行时间：38.06751 s
test openstack_500_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_500/openstack/epoch_1_step_32.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.73it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.70, False Positives: 0.77, Precision: 0.23, Recall: 0.62
程序运行时间：39.54601 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_500/openstack/epoch_2_step_32.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.72it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.61, False Positives: 0.78, Precision: 0.22, Recall: 0.81
程序运行时间：38.49508 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_500/openstack/epoch_3_step_32.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:28<00:00,  5.79it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.58, False Positives: 0.79, Precision: 0.21, Recall: 0.90
程序运行时间：36.99858 s
test openstack_1000_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_1000/openstack/epoch_1_step_63.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.73it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.66, False Positives: 0.77, Precision: 0.23, Recall: 0.76
程序运行时间：38.43526 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_1000/openstack/epoch_2_step_63.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.73it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.61, False Positives: 0.78, Precision: 0.22, Recall: 0.85
程序运行时间：38.08886 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_1000/openstack/epoch_3_step_63.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.73it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.55, False Positives: 0.80, Precision: 0.20, Recall: 0.87
程序运行时间：38.38011 s
test openstack_2000_roberta model
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_2000/openstack/epoch_1_step_126.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.72it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.71, False Positives: 0.74, Precision: 0.26, Recall: 0.75
程序运行时间：38.13970 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_2000/openstack/epoch_2_step_126.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.64, False Positives: 0.77, Precision: 0.23, Recall: 0.82
程序运行时间：38.31136 s
CUDA_VISIBLE_DEVICES=1 python RoBERTa.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/roberta_2000/openstack/epoch_3_step_126.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:28<00:00,  5.78it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.75, False Positives: 0.73, Precision: 0.27, Recall: 0.62
程序运行时间：37.69176 s
test qt_10_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_10/qt/epoch_1_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [00:57<00:00, 11.19it/s]
Test data at Threshold 0.5 -- AUc: 0.52 Accuracy: 0.08, False Positives: 0.93, Precision: 0.07, Recall: 0.97
程序运行时间：83.84403 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_10/qt/epoch_2_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [00:57<00:00, 11.17it/s]
Test data at Threshold 0.5 -- AUc: 0.65 Accuracy: 0.08, False Positives: 0.93, Precision: 0.07, Recall: 0.99
程序运行时间：83.41443 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_10/qt/epoch_3_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [00:57<00:00, 11.20it/s]
Test data at Threshold 0.5 -- AUc: 0.70 Accuracy: 0.17, False Positives: 0.92, Precision: 0.08, Recall: 0.97
程序运行时间：82.95737 s
test qt_100_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_100/qt/epoch_1_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.87it/s]
Traceback (most recent call last):
  File "CodeBERT.py", line 717, in <module>
    auc_score, A, E, P, R = evaluation_model(data=data, params=params)
  File "CodeBERT.py", line 663, in evaluation_model
    A, E, P, R=eval(all_label, all_predict, thresh=0.5)
  File "CodeBERT.py", line 504, in eval
    return (A, E, P, R)
UnboundLocalError: local variable 'A' referenced before assignment
Test data at Threshold 0.5 -- AUc: 100.0 Accuracy: 100.0, False Positives: 100.0, Precision: 100.0,Recall: 100.0
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_100/qt/epoch_2_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.92, False Positives: 0.75, Precision: 0.25, Recall: 0.02
程序运行时间：81.01229 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_100/qt/epoch_3_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.86, False Positives: 0.78, Precision: 0.22, Recall: 0.39
程序运行时间：81.61402 s
test qt_500_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_500/qt/epoch_1_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.71, False Positives: 0.85, Precision: 0.15, Recall: 0.66
程序运行时间：80.27405 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_500/qt/epoch_2_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.67, False Positives: 0.88, Precision: 0.12, Recall: 0.56
程序运行时间：80.49953 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_500/qt/epoch_3_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.79, False Positives: 0.85, Precision: 0.15, Recall: 0.42
程序运行时间：80.67818 s
test qt_1000_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_1000/qt/epoch_1_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.75, False Positives: 0.83, Precision: 0.17, Recall: 0.63
程序运行时间：81.98020 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_1000/qt/epoch_2_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.70, False Positives: 0.86, Precision: 0.14, Recall: 0.63
程序运行时间：79.41231 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_1000/qt/epoch_3_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.75, False Positives: 0.84, Precision: 0.16, Recall: 0.58
程序运行时间：78.75764 s
test qt_2000_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_2000/qt/epoch_1_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.87it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.67, False Positives: 0.85, Precision: 0.15, Recall: 0.76
程序运行时间：77.42595 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_2000/qt/epoch_2_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.88it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.67, False Positives: 0.85, Precision: 0.15, Recall: 0.77
程序运行时间：77.98539 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codebert_2000/qt/epoch_3_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:54<00:00,  5.87it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.74, False Positives: 0.83, Precision: 0.17, Recall: 0.65
程序运行时间：78.54472 s
test openstack_10_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_10/openstack/epoch_1_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:30<00:00, 10.93it/s]
Test data at Threshold 0.5 -- AUc: 0.45 Accuracy: 0.86, False Positives: 0.80, Precision: 0.20, Recall: 0.03
程序运行时间：39.36568 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_10/openstack/epoch_2_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:30<00:00, 10.93it/s]
Test data at Threshold 0.5 -- AUc: 0.50 Accuracy: 0.84, False Positives: 0.76, Precision: 0.24, Recall: 0.15
程序运行时间：40.48129 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_10/openstack/epoch_3_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:30<00:00, 10.92it/s]
Test data at Threshold 0.5 -- AUc: 0.54 Accuracy: 0.78, False Positives: 0.81, Precision: 0.19, Recall: 0.24
程序运行时间：41.99842 s
test openstack_100_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_100/openstack/epoch_1_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Test data at Threshold 0.5 -- AUc: 0.67 Accuracy: 0.12, False Positives: 0.88, Precision: 0.12, Recall: 1.00
程序运行时间：38.58302 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_100/openstack/epoch_2_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.12, False Positives: 0.88, Precision: 0.12, Recall: 1.00
程序运行时间：38.15724 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_100/openstack/epoch_3_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.28, False Positives: 0.85, Precision: 0.15, Recall: 0.98
程序运行时间：38.63416 s
test openstack_500_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_500/openstack/epoch_1_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.75it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.43, False Positives: 0.83, Precision: 0.17, Recall: 0.94
程序运行时间：38.64487 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_500/openstack/epoch_2_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.75it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.72, False Positives: 0.74, Precision: 0.26, Recall: 0.66
程序运行时间：37.56497 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_500/openstack/epoch_3_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.56, False Positives: 0.81, Precision: 0.19, Recall: 0.80
程序运行时间：38.25455 s
test openstack_1000_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_1000/openstack/epoch_1_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.61, False Positives: 0.78, Precision: 0.22, Recall: 0.84
程序运行时间：38.40849 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_1000/openstack/epoch_2_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.75it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.64, False Positives: 0.77, Precision: 0.23, Recall: 0.80
程序运行时间：38.68355 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_1000/openstack/epoch_3_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.76it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.63, False Positives: 0.78, Precision: 0.22, Recall: 0.80
程序运行时间：48.02382 s
test openstack_2000_codebert model
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_2000/openstack/epoch_1_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.74it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.66, False Positives: 0.76, Precision: 0.24, Recall: 0.80
程序运行时间：38.66643 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_2000/openstack/epoch_2_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:28<00:00,  5.80it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.60, False Positives: 0.79, Precision: 0.21, Recall: 0.82
程序运行时间：39.91271 s
CUDA_VISIBLE_DEVICES=1 python CodeBERT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codebert_2000/openstack/epoch_3_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:29<00:00,  5.75it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.53, False Positives: 0.81, Precision: 0.19, Recall: 0.83
程序运行时间：38.58455 s
test qt_10_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_10/qt/epoch_1_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.21it/s]
Test data at Threshold 0.5 -- AUc: 0.44 Accuracy: 0.09, False Positives: 0.93, Precision: 0.07, Recall: 0.95
程序运行时间：93.26447 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_10/qt/epoch_2_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.29it/s]
Test data at Threshold 0.5 -- AUc: 0.52 Accuracy: 0.11, False Positives: 0.93, Precision: 0.07, Recall: 0.90
程序运行时间：92.62309 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_10/qt/epoch_3_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.24it/s]
Test data at Threshold 0.5 -- AUc: 0.50 Accuracy: 0.24, False Positives: 0.93, Precision: 0.07, Recall: 0.71
程序运行时间：92.93912 s
test qt_100_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_100/qt/epoch_1_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.51 Accuracy: 0.52, False Positives: 0.93, Precision: 0.07, Recall: 0.49
程序运行时间：88.16328 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_100/qt/epoch_2_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.05it/s]
Test data at Threshold 0.5 -- AUc: 0.55 Accuracy: 0.88, False Positives: 0.82, Precision: 0.18, Recall: 0.17
程序运行时间：88.06455 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_100/qt/epoch_3_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.62 Accuracy: 0.90, False Positives: 0.79, Precision: 0.21, Recall: 0.14
程序运行时间：88.01817 s
test qt_500_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_500/qt/epoch_1_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.43 Accuracy: 0.19, False Positives: 0.93, Precision: 0.07, Recall: 0.76
程序运行时间：88.30086 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_500/qt/epoch_2_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.47 Accuracy: 0.19, False Positives: 0.93, Precision: 0.07, Recall: 0.82
程序运行时间：88.67877 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_500/qt/epoch_3_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.59 Accuracy: 0.26, False Positives: 0.92, Precision: 0.08, Recall: 0.84
程序运行时间：90.03051 s
test qt_1000_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_1000/qt/epoch_1_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.70 Accuracy: 0.62, False Positives: 0.87, Precision: 0.13, Recall: 0.71
程序运行时间：90.19346 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_1000/qt/epoch_2_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.05it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.55, False Positives: 0.88, Precision: 0.12, Recall: 0.80
程序运行时间：91.47665 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_1000/qt/epoch_3_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.46, False Positives: 0.89, Precision: 0.11, Recall: 0.91
程序运行时间：91.37518 s
test qt_2000_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_2000/qt/epoch_1_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.04it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.63, False Positives: 0.86, Precision: 0.14, Recall: 0.77
程序运行时间：91.63286 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_2000/qt/epoch_2_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.08it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.61, False Positives: 0.87, Precision: 0.13, Recall: 0.79
程序运行时间：92.75767 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/gpt2_2000/qt/epoch_3_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.05it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.75, False Positives: 0.82, Precision: 0.18, Recall: 0.69
程序运行时间：92.87342 s
test openstack_10_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_10/openstack/epoch_1_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.09it/s]
Test data at Threshold 0.5 -- AUc: 0.42 Accuracy: 0.78, False Positives: 0.87, Precision: 0.13, Recall: 0.14
程序运行时间：47.09128 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_10/openstack/epoch_2_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.08it/s]
Test data at Threshold 0.5 -- AUc: 0.41 Accuracy: 0.64, False Positives: 0.90, Precision: 0.10, Recall: 0.23
程序运行时间：48.22974 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_10/openstack/epoch_3_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.07it/s]
Test data at Threshold 0.5 -- AUc: 0.43 Accuracy: 0.54, False Positives: 0.91, Precision: 0.09, Recall: 0.30
程序运行时间：46.58519 s
test openstack_100_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_100/openstack/epoch_1_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  5.03it/s]
Test data at Threshold 0.5 -- AUc: 0.54 Accuracy: 0.82, False Positives: 0.80, Precision: 0.20, Recall: 0.16
程序运行时间：42.64039 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_100/openstack/epoch_2_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.95it/s]
Test data at Threshold 0.5 -- AUc: 0.59 Accuracy: 0.73, False Positives: 0.80, Precision: 0.20, Recall: 0.39
程序运行时间：43.69522 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_100/openstack/epoch_3_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.94it/s]
Test data at Threshold 0.5 -- AUc: 0.61 Accuracy: 0.62, False Positives: 0.82, Precision: 0.18, Recall: 0.59
程序运行时间：44.12933 s
test openstack_500_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_500/openstack/epoch_1_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.95it/s]
Test data at Threshold 0.5 -- AUc: 0.58 Accuracy: 0.59, False Positives: 0.85, Precision: 0.15, Recall: 0.52
程序运行时间：44.73190 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_500/openstack/epoch_2_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.94it/s]
Test data at Threshold 0.5 -- AUc: 0.59 Accuracy: 0.44, False Positives: 0.85, Precision: 0.15, Recall: 0.74
程序运行时间：44.26684 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_500/openstack/epoch_3_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.95it/s]
Test data at Threshold 0.5 -- AUc: 0.65 Accuracy: 0.83, False Positives: 0.76, Precision: 0.24, Recall: 0.20
程序运行时间：44.09944 s
test openstack_1000_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_1000/openstack/epoch_1_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.68 Accuracy: 0.37, False Positives: 0.85, Precision: 0.15, Recall: 0.91
程序运行时间：42.40453 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_1000/openstack/epoch_2_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.93it/s]
Test data at Threshold 0.5 -- AUc: 0.68 Accuracy: 0.57, False Positives: 0.82, Precision: 0.18, Recall: 0.71
程序运行时间：43.73417 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_1000/openstack/epoch_3_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.95it/s]
Test data at Threshold 0.5 -- AUc: 0.70 Accuracy: 0.60, False Positives: 0.81, Precision: 0.19, Recall: 0.67
程序运行时间：44.56606 s
test openstack_2000_gpt2 model
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_2000/openstack/epoch_1_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.95it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.39, False Positives: 0.84, Precision: 0.16, Recall: 0.94
程序运行时间：43.62466 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_2000/openstack/epoch_2_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.94it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.62, False Positives: 0.78, Precision: 0.22, Recall: 0.80
程序运行时间：43.53425 s
CUDA_VISIBLE_DEVICES=1 python GPT2.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/gpt2_2000/openstack/epoch_3_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.95it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.65, False Positives: 0.77, Precision: 0.23, Recall: 0.78
程序运行时间：44.08476 s
test qt_10_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_10/qt/epoch_1_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.24it/s]
Test data at Threshold 0.5 -- AUc: 0.63 Accuracy: 0.42, False Positives: 0.91, Precision: 0.09, Recall: 0.75
程序运行时间：101.5399 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_10/qt/epoch_2_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.21it/s]
Test data at Threshold 0.5 -- AUc: 0.64 Accuracy: 0.45, False Positives: 0.91, Precision: 0.09, Recall: 0.75
程序运行时间：100.3750 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_10/qt/epoch_3_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:08<00:00,  9.36it/s]
Test data at Threshold 0.5 -- AUc: 0.65 Accuracy: 0.48, False Positives: 0.90, Precision: 0.10, Recall: 0.73
程序运行时间：99.66721 s
test qt_100_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_100/qt/epoch_1_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.53 Accuracy: 0.41, False Positives: 0.92, Precision: 0.08, Recall: 0.63
程序运行时间：95.20658 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_100/qt/epoch_2_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.60 Accuracy: 0.60, False Positives: 0.90, Precision: 0.10, Recall: 0.56
程序运行时间：95.86418 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_100/qt/epoch_3_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.08it/s]
Test data at Threshold 0.5 -- AUc: 0.64 Accuracy: 0.69, False Positives: 0.88, Precision: 0.12, Recall: 0.53
程序运行时间：91.28785 s
test qt_500_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_500/qt/epoch_1_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.75, False Positives: 0.83, Precision: 0.17, Recall: 0.61
程序运行时间：95.07999 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_500/qt/epoch_2_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.72, False Positives: 0.84, Precision: 0.16, Recall: 0.68
程序运行时间：96.75807 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_500/qt/epoch_3_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.01it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.74, False Positives: 0.83, Precision: 0.17, Recall: 0.67
程序运行时间：94.77507 s
test qt_1000_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_1000/qt/epoch_1_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.75, False Positives: 0.83, Precision: 0.17, Recall: 0.63
程序运行时间：97.04788 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_1000/qt/epoch_2_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.73, False Positives: 0.83, Precision: 0.17, Recall: 0.68
程序运行时间：98.25944 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_1000/qt/epoch_3_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.74, False Positives: 0.83, Precision: 0.17, Recall: 0.70
程序运行时间：98.36339 s
test qt_2000_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_2000/qt/epoch_1_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.70, False Positives: 0.84, Precision: 0.16, Recall: 0.74
程序运行时间：99.16587 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_2000/qt/epoch_2_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:03<00:00,  5.06it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.69, False Positives: 0.84, Precision: 0.16, Recall: 0.76
程序运行时间：99.13175 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/codegpt_2000/qt/epoch_3_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
5141 5141 5141
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:04<00:00,  5.02it/s]
Test data at Threshold 0.5 -- AUc: 0.81 Accuracy: 0.73, False Positives: 0.82, Precision: 0.18, Recall: 0.75
程序运行时间：101.4829 s
test openstack_10_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_10/openstack/epoch_1_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.04it/s]
Test data at Threshold 0.5 -- AUc: 0.52 Accuracy: 0.48, False Positives: 0.87, Precision: 0.13, Recall: 0.56
程序运行时间：49.87213 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_10/openstack/epoch_2_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.05it/s]
Test data at Threshold 0.5 -- AUc: 0.56 Accuracy: 0.55, False Positives: 0.85, Precision: 0.15, Recall: 0.56
程序运行时间：50.16142 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_10/openstack/epoch_3_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.18it/s]
Test data at Threshold 0.5 -- AUc: 0.58 Accuracy: 0.60, False Positives: 0.84, Precision: 0.16, Recall: 0.54
程序运行时间：49.81740 s
test openstack_100_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_100/openstack/epoch_1_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.92it/s]
Test data at Threshold 0.5 -- AUc: 0.54 Accuracy: 0.44, False Positives: 0.86, Precision: 0.14, Recall: 0.66
程序运行时间：47.41505 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_100/openstack/epoch_2_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.93it/s]
Test data at Threshold 0.5 -- AUc: 0.62 Accuracy: 0.56, False Positives: 0.83, Precision: 0.17, Recall: 0.64
程序运行时间：47.09704 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_100/openstack/epoch_3_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.92it/s]
Test data at Threshold 0.5 -- AUc: 0.66 Accuracy: 0.64, False Positives: 0.81, Precision: 0.19, Recall: 0.62
程序运行时间：47.23787 s
test openstack_500_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_500/openstack/epoch_1_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.93it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.72, False Positives: 0.77, Precision: 0.23, Recall: 0.54
程序运行时间：46.58975 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_500/openstack/epoch_2_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.92it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.65, False Positives: 0.78, Precision: 0.22, Recall: 0.73
程序运行时间：48.12097 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_500/openstack/epoch_3_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.99it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.67, False Positives: 0.77, Precision: 0.23, Recall: 0.71
程序运行时间：48.07767 s
test openstack_1000_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_1000/openstack/epoch_1_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.93it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.68, False Positives: 0.77, Precision: 0.23, Recall: 0.69
程序运行时间：45.35831 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_1000/openstack/epoch_2_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.93it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.65, False Positives: 0.78, Precision: 0.22, Recall: 0.77
程序运行时间：45.50659 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_1000/openstack/epoch_3_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.92it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.66, False Positives: 0.77, Precision: 0.23, Recall: 0.75
程序运行时间：46.36914 s
test openstack_2000_codegpt model
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_2000/openstack/epoch_1_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.92it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.68, False Positives: 0.76, Precision: 0.24, Recall: 0.73
程序运行时间：45.53100 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_2000/openstack/epoch_2_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.95it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.67, False Positives: 0.76, Precision: 0.24, Recall: 0.75
程序运行时间：45.03821 s
CUDA_VISIBLE_DEVICES=1 python CodeGPT.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/codegpt_2000/openstack/epoch_3_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length at tokenizer: 256
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 max length for code tokenizer: 120
2661 2661 2661
Some weights of the model checkpoint at microsoft/CodeGPT-small-java were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:33<00:00,  4.92it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.71, False Positives: 0.75, Precision: 0.25, Recall: 0.69
程序运行时间：46.04500 s
test qt_10_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_10/qt/epoch_1_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:13<00:00,  8.73it/s]
Test data at Threshold 0.5 -- AUc: 0.67 Accuracy: 0.45, False Positives: 0.90, Precision: 0.10, Recall: 0.78
程序运行时间：113.0965 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_10/qt/epoch_2_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:12<00:00,  8.84it/s]
Test data at Threshold 0.5 -- AUc: 0.70 Accuracy: 0.66, False Positives: 0.87, Precision: 0.13, Recall: 0.64
程序运行时间：110.8166 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_10/qt/epoch_3_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:13<00:00,  8.74it/s]
Test data at Threshold 0.5 -- AUc: 0.71 Accuracy: 0.78, False Positives: 0.84, Precision: 0.16, Recall: 0.45
程序运行时间：110.2520 s
test qt_100_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_100/qt/epoch_1_step_7.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:08<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.71 Accuracy: 0.77, False Positives: 0.84, Precision: 0.16, Recall: 0.51
程序运行时间：103.7850 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_100/qt/epoch_2_step_7.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.77it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.77, False Positives: 0.83, Precision: 0.17, Recall: 0.54
程序运行时间：99.71213 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_100/qt/epoch_3_step_7.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:08<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.75, False Positives: 0.83, Precision: 0.17, Recall: 0.61
程序运行时间：104.3496 s
test qt_500_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_500/qt/epoch_1_step_32.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.75it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.69, False Positives: 0.85, Precision: 0.15, Recall: 0.70
程序运行时间：101.5482 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_500/qt/epoch_2_step_32.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:08<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.71, False Positives: 0.84, Precision: 0.16, Recall: 0.72
程序运行时间：104.7828 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_500/qt/epoch_3_step_32.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:08<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.69, False Positives: 0.84, Precision: 0.16, Recall: 0.75
程序运行时间：105.8878 s
test qt_1000_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_1000/qt/epoch_1_step_63.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.78it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.70, False Positives: 0.84, Precision: 0.16, Recall: 0.74
程序运行时间：101.6124 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_1000/qt/epoch_2_step_63.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:08<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.76, False Positives: 0.82, Precision: 0.18, Recall: 0.66
程序运行时间：105.1105 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_1000/qt/epoch_3_step_63.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:08<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.65, False Positives: 0.85, Precision: 0.15, Recall: 0.79
程序运行时间：103.6970 s
test qt_2000_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_2000/qt/epoch_1_step_126.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:05<00:00,  4.89it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.72, False Positives: 0.83, Precision: 0.17, Recall: 0.71
程序运行时间：95.88329 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_2000/qt/epoch_2_step_126.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.63, False Positives: 0.86, Precision: 0.14, Recall: 0.82
程序运行时间：101.8150 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/bart_2000/qt/epoch_3_step_126.pt -dictionary_data ../data/qt_dict.pkl
predicting lora-codebert model
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 120
5141 5141 5141
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:05<00:00,  4.93it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.63, False Positives: 0.86, Precision: 0.14, Recall: 0.81
程序运行时间：95.88849 s
test openstack_10_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_10/openstack/epoch_1_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.14it/s]
Test data at Threshold 0.5 -- AUc: 0.36 Accuracy: 0.14, False Positives: 0.88, Precision: 0.12, Recall: 0.93
程序运行时间：47.59179 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_10/openstack/epoch_2_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.14it/s]
Test data at Threshold 0.5 -- AUc: 0.45 Accuracy: 0.18, False Positives: 0.88, Precision: 0.12, Recall: 0.87
程序运行时间：47.91447 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_10/openstack/epoch_3_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:36<00:00,  9.18it/s]
Test data at Threshold 0.5 -- AUc: 0.53 Accuracy: 0.29, False Positives: 0.88, Precision: 0.12, Recall: 0.78
程序运行时间：48.30684 s
test openstack_100_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_100/openstack/epoch_1_step_7.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.69 Accuracy: 0.87, False Positives: 0.70, Precision: 0.30, Recall: 0.06
程序运行时间：46.21219 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_100/openstack/epoch_2_step_7.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.72 Accuracy: 0.83, False Positives: 0.71, Precision: 0.29, Recall: 0.23
程序运行时间：46.29008 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_100/openstack/epoch_3_step_7.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.79, False Positives: 0.74, Precision: 0.26, Recall: 0.40
程序运行时间：46.41733 s
test openstack_500_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_500/openstack/epoch_1_step_32.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.57, False Positives: 0.80, Precision: 0.20, Recall: 0.85
程序运行时间：46.19344 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_500/openstack/epoch_2_step_32.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.63, False Positives: 0.78, Precision: 0.22, Recall: 0.80
程序运行时间：45.89761 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_500/openstack/epoch_3_step_32.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.64, False Positives: 0.77, Precision: 0.23, Recall: 0.81
程序运行时间：46.82383 s
test openstack_1000_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_1000/openstack/epoch_1_step_63.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.48, False Positives: 0.82, Precision: 0.18, Recall: 0.94
程序运行时间：46.54596 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_1000/openstack/epoch_2_step_63.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.66, False Positives: 0.77, Precision: 0.23, Recall: 0.73
程序运行时间：46.23996 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_1000/openstack/epoch_3_step_63.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.68, False Positives: 0.77, Precision: 0.23, Recall: 0.66
程序运行时间：46.23065 s
test openstack_2000_bart model
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_2000/openstack/epoch_1_step_126.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.bias', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.69, False Positives: 0.76, Precision: 0.24, Recall: 0.70
程序运行时间：46.72101 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_2000/openstack/epoch_2_step_126.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.71, False Positives: 0.75, Precision: 0.25, Recall: 0.67
程序运行时间：46.88660 s
CUDA_VISIBLE_DEVICES=1 python plbart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/bart_2000/openstack/epoch_3_step_126.pt -dictionary_data ../data/openstack_dict.pkl
predicting lora-codebert model
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 120
2661 2661 2661
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:34<00:00,  4.87it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.74, False Positives: 0.73, Precision: 0.27, Recall: 0.69
程序运行时间：46.56846 s
test qt_10_plbart model
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_10/qt/epoch_1_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.26it/s]
Test data at Threshold 0.5 -- AUc: 0.61 Accuracy: 0.93, False Positives: 0.75, Precision: 0.25, Recall: 0.01
程序运行时间：103.5474 s
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_10/qt/epoch_2_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.27it/s]
Test data at Threshold 0.5 -- AUc: 0.71 Accuracy: 0.92, False Positives: 0.78, Precision: 0.22, Recall: 0.03
程序运行时间：102.6391 s
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_10/qt/epoch_3_step_2.pt -dictionary_data ../data/qt_dict.pkl -batch_size 8
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643/643 [01:09<00:00,  9.27it/s]
Test data at Threshold 0.5 -- AUc: 0.73 Accuracy: 0.91, False Positives: 0.76, Precision: 0.24, Recall: 0.10
程序运行时间：103.5816 s
test qt_100_plbart model
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_100/qt/epoch_1_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.67 Accuracy: 0.93, False Positives: 0.80, Precision: 0.20, Recall: 0.00
程序运行时间：100.6016 s
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_100/qt/epoch_2_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.92, False Positives: 0.75, Precision: 0.25, Recall: 0.06
程序运行时间：100.6710 s
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_100/qt/epoch_3_step_7.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.85it/s]
Test data at Threshold 0.5 -- AUc: 0.76 Accuracy: 0.88, False Positives: 0.79, Precision: 0.21, Recall: 0.24
程序运行时间：100.4304 s
test qt_500_plbart model
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_500/qt/epoch_1_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.76, False Positives: 0.83, Precision: 0.17, Recall: 0.61
程序运行时间：101.2693 s
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_500/qt/epoch_2_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.86it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.72, False Positives: 0.84, Precision: 0.16, Recall: 0.69
程序运行时间：100.6361 s
CUDA_VISIBLE_DEVICES=1 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_500/qt/epoch_3_step_32.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.85it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.73, False Positives: 0.83, Precision: 0.17, Recall: 0.68
程序运行时间：102.8859 s
test qt_1000_plbart model
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_1000/qt/epoch_1_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:06<00:00,  4.82it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.71, False Positives: 0.84, Precision: 0.16, Recall: 0.72
程序运行时间：76.34671 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_1000/qt/epoch_2_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.80it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.75, False Positives: 0.82, Precision: 0.18, Recall: 0.68
程序运行时间：76.82221 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_1000/qt/epoch_3_step_63.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.80it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.69, False Positives: 0.84, Precision: 0.16, Recall: 0.76
程序运行时间：83.29046 s
test qt_2000_plbart model
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_2000/qt/epoch_1_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.79it/s]
Test data at Threshold 0.5 -- AUc: 0.78 Accuracy: 0.73, False Positives: 0.83, Precision: 0.17, Recall: 0.70
程序运行时间：81.01051 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_2000/qt/epoch_2_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.80it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.71, False Positives: 0.83, Precision: 0.17, Recall: 0.75
程序运行时间：82.19791 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/qt_data/qt_test.pkl -load_model few_shot_snapshot/plbart_2000/qt/epoch_3_step_126.pt -dictionary_data ../data/qt_dict.pkl
5141 5141
 max length at tokenizer: 256
 max length for code tokenizer: 128
5141 5141 5141
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [01:07<00:00,  4.80it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.58, False Positives: 0.87, Precision: 0.13, Recall: 0.86

test openstack_10_plbart model
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_10/openstack/epoch_1_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:37<00:00,  8.97it/s]
Test data at Threshold 0.5 -- AUc: 0.67 Accuracy: 0.54, False Positives: 0.82, Precision: 0.18, Recall: 0.75
程序运行时间：51.77834 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_10/openstack/epoch_2_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:37<00:00,  8.93it/s]
Test data at Threshold 0.5 -- AUc: 0.72 Accuracy: 0.67, False Positives: 0.77, Precision: 0.23, Recall: 0.70
程序运行时间：56.36127 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_10/openstack/epoch_3_step_2.pt -dictionary_data ../data/openstack_dict.pkl -batch_size 8
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 333/333 [00:37<00:00,  8.93it/s]
Test data at Threshold 0.5 -- AUc: 0.74 Accuracy: 0.70, False Positives: 0.76, Precision: 0.24, Recall: 0.65
程序运行时间：52.42012 s
test openstack_100_plbart model
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_100/openstack/epoch_1_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.74it/s]
Test data at Threshold 0.5 -- AUc: 0.70 Accuracy: 0.12, False Positives: 0.88, Precision: 0.12, Recall: 1.00
程序运行时间：50.83142 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_100/openstack/epoch_2_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.38, False Positives: 0.84, Precision: 0.16, Recall: 0.97
程序运行时间：50.55465 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_100/openstack/epoch_3_step_7.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.50, False Positives: 0.81, Precision: 0.19, Recall: 0.91
程序运行时间：51.75123 s
test openstack_500_plbart model
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_500/openstack/epoch_1_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.72it/s]
Test data at Threshold 0.5 -- AUc: 0.75 Accuracy: 0.70, False Positives: 0.76, Precision: 0.24, Recall: 0.64
程序运行时间：52.70943 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_500/openstack/epoch_2_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.68, False Positives: 0.76, Precision: 0.24, Recall: 0.74
程序运行时间：52.66671 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_500/openstack/epoch_3_step_32.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.70, False Positives: 0.75, Precision: 0.25, Recall: 0.73
程序运行时间：51.98571 s
test openstack_1000_plbart model
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_1000/openstack/epoch_1_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.71, False Positives: 0.75, Precision: 0.25, Recall: 0.68
程序运行时间：53.63318 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_1000/openstack/epoch_2_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.69, False Positives: 0.75, Precision: 0.25, Recall: 0.75
程序运行时间：52.43628 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_1000/openstack/epoch_3_step_63.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.80 Accuracy: 0.68, False Positives: 0.75, Precision: 0.25, Recall: 0.80
程序运行时间：52.01477 s
test openstack_2000_plbart model
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_2000/openstack/epoch_1_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.63, False Positives: 0.77, Precision: 0.23, Recall: 0.88
程序运行时间：53.34142 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_2000/openstack/epoch_2_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.79 Accuracy: 0.75, False Positives: 0.72, Precision: 0.28, Recall: 0.65
程序运行时间：59.36231 s
CUDA_VISIBLE_DEVICES=0 python bart.py -predict -pred_data ../data/openstack_data/openstack_test.pkl -load_model few_shot_snapshot/plbart_2000/openstack/epoch_3_step_126.pt -dictionary_data ../data/openstack_dict.pkl
2661 2661
 max length at tokenizer: 256
 max length for code tokenizer: 128
2661 2661 2661
Some weights of the model checkpoint at plbart were not used when initializing PLBartModel: ['lm_head.weight', 'final_logits_bias']
- This IS expected if you are initializing PLBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PLBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [00:35<00:00,  4.73it/s]
Test data at Threshold 0.5 -- AUc: 0.77 Accuracy: 0.66, False Positives: 0.77, Precision: 0.23, Recall: 0.74
程序运行时间：59.33355 s
